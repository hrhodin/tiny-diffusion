{
 "cells": [
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "from locale import normalize\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "cell_type": "code",
   "metadata": {},
   "source": [
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.optim as optim\n",
    "import losses, datasets, ddpm, models, geometry"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "s_0 = torch.tensor([1.0])\n",
    "z_0 = torch.tensor([1.0])"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# Create grid\n",
    "grid_res = 100\n",
    "z_vals = torch.linspace(0, 2, grid_res)\n",
    "s_vals = torch.linspace(0, 2, grid_res)\n",
    "\n",
    "loss_grid = torch.zeros(grid_res, grid_res)\n",
    "for i, z in enumerate(z_vals):\n",
    "    for j, s in enumerate(s_vals):\n",
    "        loss_grid[j, i] = losses.circle_loss(z, s, z_0, s_0)\n",
    "\n",
    "# Plot\n",
    "plt.imshow(loss_grid.numpy(), extent=[0, 2, 0, 2], origin='lower', aspect='auto', cmap='viridis')\n",
    "plt.colorbar(label=\"Loss\")\n",
    "plt.xlabel(\"z\")\n",
    "plt.ylabel(\"s\")\n",
    "plt.title(\"Loss surface for fwd_loss(z, s)\")\n",
    "plt.show()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Plain regression"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": ""
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": "# Supervised diffusion"
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "import models, geometry, datasets, losses, ddpm, example_2Dsimple\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "def main():\n",
    "    # Linear\n",
    "    T = 100\n",
    "    bs = 1000\n",
    "\n",
    "    betas = ddpm.linear_beta_schedule(T)\n",
    "    a, abar = ddpm.alpha_bar(betas)  # \\bar{Î±}_t for t=1..T\n",
    "\n",
    "    plt.plot(range(1, T + 1), a, label=r\"$'\\alpha'_t$ (Linear Beta)\")\n",
    "    plt.plot(range(1, T + 1), betas, label=r\"$'\\beta'_t$ (Linear Beta)\")\n",
    "    plt.plot(range(1, T + 1), abar, label=r\"$\\bar{\\alpha}_t$ (Linear Beta)\")\n",
    "    plt.xlabel(\"Timestep\")\n",
    "    plt.ylabel(r\"$\\bar{\\alpha}_t$\")\n",
    "    plt.title(\"Alpha Bar vs Timestep (Linear Beta Schedule)\")\n",
    "    plt.grid(True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    model_diff = models.SimpleNN(IO=2, N=10, C=1+1, bound=True)\n",
    "\n",
    "    # Loss & optimizer\n",
    "    optimizer = optim.Adam(model_diff.parameters(), lr=0.01)\n",
    "\n",
    "    dataset = datasets.Dataset2DSphere(length=20000*bs, seed=8)\n",
    "    loader = DataLoader(\n",
    "        dataset,\n",
    "        batch_size=bs,\n",
    "        shuffle=True,  # reshuffle each epoch\n",
    "        num_workers=0,  # >0 for parallel loading\n",
    "        pin_memory=False  # True if training on GPU\n",
    "    )\n",
    "\n",
    "    # Minimalistic loop\n",
    "    log = []\n",
    "    for iter, batch in enumerate(loader):\n",
    "        s_0, z_0, p_0, s_1, z_1, p_1 = batch['s_0'], batch['z_0'], batch['p_0'], batch['s_1'], batch['z_1'], batch['p_1']\n",
    "\n",
    "        # warmup\n",
    "        t = torch.ones((bs,), dtype=int) * (T - 1)\n",
    "        #z_t = 2 * torch.randn(bs, 2)  # HACK 100 samples, 2 features\n",
    "        x_1 = torch.cat([s_1, z_1], dim=-1)\n",
    "        x_eps = model_diff(x_1, [p_0, t])  # diffusion\n",
    "        x_t, x_var = ddpm.denoising_step(a, betas, abar, t, x_1, x_eps)\n",
    "\n",
    "        # sample step times t and add noise up to step t\n",
    "        t = torch.randint(0, T, (bs,))\n",
    "        x_t, eps_gt = ddpm.q_sample(abar, x_t, t)\n",
    "        x_eps = model_diff(x_t, [p_0, t])  # diffusion\n",
    "        x_t, x_var = ddpm.denoising_step(a, betas, abar, t, x_t, x_eps)\n",
    "\n",
    "        if 1:  # deterministic process\n",
    "            diff = losses.circle_loss(x_t[:, 0], x_t[:, 1], z_0, s_0)\n",
    "        else:  # random process (TODO: run a few steps of this before computing the loss)\n",
    "            N_std_noise = torch.randn_like(x_t)\n",
    "            print(\"X\", z_var.shape, z_mean.shape, N_std_noise.shape)\n",
    "            x_prev = x_t + torch.sqrt(x_var) * N_std_noise\n",
    "            diff = losses.circle_loss(x_prev[:, 0], x_prev[:, 1], z_0, s_0)\n",
    "\n",
    "        loss = diff.mean()\n",
    "        optimizer.zero_grad()  # reset gradients\n",
    "        loss.backward()  # backprop\n",
    "        optimizer.step()  # update weights\n",
    "        log.append(loss.item())\n",
    "        reference = math.pow(10.0, math.floor(math.log10(iter + 1)))\n",
    "        if (iter + 1) % reference == 0:\n",
    "            print(f\"Iter {iter + 1}, loss = {loss.item():.4f}\")\n",
    "    losses.plot_losses(log)\n",
    "\n",
    "    # now sample from the learned (conditional) distribution\n",
    "    s_0 = torch.FloatTensor([1.0]) #torch.rand(1)  # 100 samples, 2 features\n",
    "    z_0 = torch.FloatTensor([1.0]) # torch.rand(1)  # class labels {0,1}\n",
    "    p_0 = geometry.project_1D(z_0, s_0).expand(p_1.shape)\n",
    "\n",
    "    x_1 = torch.randn(x_eps.shape)\n",
    "    samples = ddpm.ddpm_sample(model_diff, x_1, [p_0], betas=betas, abar=abar)\n",
    "    print(samples)\n",
    "\n",
    "    plot_2d_samples(samples, p_0)\n",
    "    \n",
    "    \n",
    "    example_2Dsimple.plot_2d_samples(samples, p_0)\n",
    "    example_2Dsimple.plot_2d_samples(samples.abs(), p_0)\n",
    "\n",
    "    pass\n",
    "\n",
    "if __name__ == '__main__':\n",
    "    main()"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "# model_untrained = SimpleNN(IO=2, N=10, C=C, bound=True) # leads to a spread around 0, often with a slight shift to one side\n",
    "\n",
    "\n",
    "samples"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": [
    "z = z_mean\n",
    "num = 5\n",
    "print(f\" z={z[:num, 0]},\\n s={z[:num, 1]} \\n p={project_1D(z[:num, 0], z[:num, 1]).abs()},\\n l={-circle_iou_concentric(project_1D(z[:num, 0], z[:num, 1]), s_0)},\\n gt={s_0}\")"
   ],
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "z.shap    ",
   "outputs": [],
   "execution_count": null
  },
  {
   "metadata": {},
   "cell_type": "code",
   "source": "",
   "outputs": [],
   "execution_count": null
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
